{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import json\n",
    "import os\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_batch(data ,max_len,word_to_ix, tag_to_ix):\n",
    "    seqs = [i[0] for i in data]\n",
    "    tags = [i[1] for i in data]\n",
    "    #max_len = max([len(seq) for seq in seqs])\n",
    "    seqs_pad=[]\n",
    "    tags_pad=[]\n",
    "    for seq,tag in zip(seqs, tags):\n",
    "        if len(seq)<max_len:\n",
    "            seq_pad = list(chain.from_iterable(seq)) + ['[PAD]'] * (max_len-len(seq))\n",
    "            tag_pad = tag + ['[PAD]'] * (max_len-len(tag))\n",
    "        else:\n",
    "            seq_pad = list(chain.from_iterable(seq))[:100]\n",
    "            tag_pad = tag[:100]\n",
    "        seqs_pad.append(seq_pad)\n",
    "        tags_pad.append(tag_pad)\n",
    "    idxs_pad = torch.tensor([[word_to_ix[w] for w in seq] for seq in seqs_pad], dtype=torch.long)\n",
    "    tags_pad = torch.tensor([[tag_to_ix[t] for t in tag] for tag in tags_pad], dtype=torch.long)\n",
    "    return idxs_pad, tags_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "PAD_TAG = \"<PAD>\"\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "epoch = 10\n",
    "bs = 100\n",
    "max_len = 100\n",
    "datafile = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./People's_Daily/wdata.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_data = data[:int(0.8*len(data))]\n",
    "test_data = data[int(0.8*len(data)):int(0.9*len(data))]\n",
    "valid_data = data[int(0.9*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./People's_Daily/vocab.json\") as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./People's_Daily/label.json\") as f:\n",
    "    label = json.load(f)\n",
    "label.append(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "id2word = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2id[word] = i\n",
    "    id2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {}\n",
    "id2label = {}\n",
    "for i,lb in enumerate(label):\n",
    "    label2id[lb] = i\n",
    "    id2label[i] = lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_pad, tags_pad = prepare_sequence_batch(train_data,max_len,word2id,label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((idxs_pad, tags_pad),\"./train_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_pad, tags_pad = prepare_sequence_batch(test_data,max_len,word2id,label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((idxs_pad, tags_pad),\"./test_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_pad, tags_pad = prepare_sequence_batch(valid_data,max_len,word2id,label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((idxs_pad, tags_pad),\"./valid_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_pad, tags_pad = torch.load(\"./People's_Daily/train_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1940,  339,   91,  ...,    0,    0,    0],\n",
       "        [ 872, 1535,  872,  ...,    0,    0,    0],\n",
       "        [4151, 3567, 3580,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 316, 3472, 2893,  ..., 4418, 3285, 1570],\n",
       "        [ 316, 3472, 1985,  ..., 4239, 4029, 3567],\n",
       "        [ 316, 3472, 1007,  ...,  316, 3472, 1007]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
